摘要
大语言模型（LLMs）展示了显著的能力，但面临着幻觉、过时知识和不透明、不可追溯的推理过程等挑战。检索增强生成（RAG）通过整合外部数据库的知识，成为了一个有前景的解决方案。这增强了模型的准确性和可信度，特别是在知识密集型任务中，并允许持续的知识更新和领域特定信息的整合。RAG将LLMs的内在知识与外部数据库的广阔、动态存储库相结合。这篇综合性的回顾论文详细考察了RAG范式的进展，包括Naive RAG、Advanced RAG和Modular RAG。它仔细审查了RAG框架的三部分基础，即检索、生成和增强技术。本文强调了这些关键组件中嵌入的最新技术，提供了对RAG系统进展的深刻理解。此外，本文介绍了评估RAG模型的指标和最新的评估框架。最后，本文概述了研究的前景，包括挑战的识别、多模态性的扩展以及RAG基础设施和生态系统的进步。

1 引言
大语言模型（如GPT系列和LLama系列，以及其他模型如Gemini）在自然语言处理方面取得了显著的成功，展示了在各种基准测试（benchmark）中的卓越性能。但LLMs在处理领域特定或高度专业化的query时仍会表现出明显的局限性。一个常见问题是会生成不正确的信息（或称为“幻觉”），这种问题在query超出模型训练数据范围或需要最新信息时表现得尤为明显。这些局限性导致了在没有额外保障的情况下，将LLMs作为黑盒解决方案部署在现实生产环境中的不切实际性。缓解这些局限性的一个有前途的方法是检索增强生成（RAG），它将外部数据检索整合到生成过程中，从而增强模型提供准确和相关响应的能力。

RAG，由Lewis等在2020年中期引入，是LLMs领域内的一个增强生成任务的范式。具体来说，RAG涉及一个初始的检索步骤，其中LLMs查询外部数据源以获取相关信息，然后再回答query或生成文本。这个过程不仅为后续的生成阶段提供信息，而且确保模型的回答是基于检索到的信息，显著提高了输出的准确性和相关性。

在推理阶段从知识库动态检索信息，使RAG能够解决生成“幻觉”的问题。将RAG整合到LLMs中已经迅速被采用，并成为完善聊天机器人能力和使LLMs更适合实际应用的关键技术。


图1：具有代表性作品的RAG研发技术树
RAG的演变轨迹跨越了四个不同的阶段，如图1所示。在其2017年的起源阶段，随着Transformer架构的出现，主要重点是通过预训练模型（PTM）吸收额外的知识来增强语言模型。这一时期见证了RAG的基础努力主要集中在优化预训练方法上。

在这一初始阶段之后，RAG研究相对休眠了一段时间，直到chatGPT的出现。随后，LLMs进入了前沿，社区的焦点转向利用LLMs的能力以获得更高的可控性并满足不断变化的需求。因此，RAG的大部分努力集中在推理上，只有少数致力于微调过程。

在这个初始阶段之后，在chatGPT出现之前，有一段相对的休眠期，在此期间，对RAG的相关研究进展甚微。随后，chatGPT的出现标志着这一发展轨迹的关键时刻，将LLMs推向了前沿。社区的焦点转向利用LLMs的能力，以达到更高的可控性和解决不断变化的需求。因此，RAG的大部分努力都集中在推理上，少数致力于微调过程。随着LLM技术的不断发展，特别是GPT-4的引入，RAG技术的前景发生了重大变化。重点发展成为一种混合方法，结合RAG和微调的优势，以及专门的少数人继续专注于优化预训练方法。
尽管RAG研究发展迅速，但该领域缺乏系统的整合和抽象，这对理解RAG进展的全面前景提出了挑战。本调查旨在概述整个RAG过程，并通过提供对LLMs检索增强的彻底检查，涵盖RAG研究的当前和未来方向。

因此，本文旨在对RAG的技术原理、发展历史、内容，特别是LLMs出现后的相关方法和应用，以及RAG的评价方法和应用场景进行全面的总结和整理。试图对现有的RAG技术进行全面的概述和分析，并对未来的发展方法提出结论和展望。本调查旨在使读者和从业者对大模型和检索增强有一个全面和系统的理解，阐明检索增强的进展和关键技术，阐明各种技术的优点和局限性以及它们的适用背景，并预测潜在的未来发展。

本文贡献如下:

文章对最先进的RAG进行了全面而系统的回顾，通过包括幼稚RAG、高级RAG和Modular RAG在内的范式描述了其演变。这篇综述的背景下，更广泛的范围内的LLMs研究RAG的景观。
文章确定并讨论了RAG过程中不可或缺的核心技术，特别关注“检索”，“生成器”和“增强”方面，并深入研究了它们的协同作用，阐明了这些组件如何复杂地协作以形成一个有凝聚力和有效的RAG框架。
文章为RAG构建了一个全面的评估框架，概述了评估目标和指标。我们的对比分析从不同的角度阐明了RAG与微调相比的优缺点。此外，我们预测了RAG的未来方向，强调潜在的增强以应对当前的挑战，扩展到多模态设置，以及其生态系统的发展。
第二节和第三节对RAG进行了定义，并详细介绍了RAG的发展过程。第4节到第6节探讨了核心组件——检索、生成和增”——重点介绍了各种嵌入式技术。第7节重点介绍RAG的评估体系。第8节将RAG与其他LLM优化方法进行了比较，并提出了其可能的发展方向。全文在第9节结束。

2 定义
RAG的定义可以从其工作流程中总结。图2描绘了一个典型的RAG应用工作流程。在这种情况下，用户询问ChatGPT关于一个最近引起公众讨论的高调事件（即OpenAI的CEO突然被解雇和复职）。ChatGPT作为最著名和广泛使用的LLM，受限于其预训练数据，缺乏对最近事件的知识。RAG通过从外部知识库检索最新的文件摘要来解决这一差距。这些文章，加上初始query，然后被合并成一个丰富的提示，使ChatGPT能够合成一个知情的回应。这个例子说明了RAG过程，展示了它如何通过实时信息检索增强模型的响应。


图2：应用于问答的RAG流程的代表性实例

技术上，RAG通过各种创新方法丰富了各种关键query，如“检索什么”、“何时检索”和“如何使用检索到的信息”。对于“检索什么”，研究已经从简单的标记[Khandelwal et al., 2019]和实体检索[Nishikawa et al., 2022]发展到更复杂的结构，如块（chunks）检索[Ram et al., 2023]和知识图谱[Kang et al., 2023]，研究集中在检索的粒度和数据结构的水平上。粗粒度带来更多的信息，但精度较低，结构化文本提供更多的信息，但牺牲了效率。至于“何时检索”，已经从单一[Wang et al., 2023e, Shi et al., 2023]发展到自适应[Jiang et al., 2023b, Huang et al., 2023]和多次检索[Izacard et al., 2022]的策略。检索频率高会带来更多的信息和更低的效率。关于“如何使用检索到的数据“，已经开发了各种级别的模型架构集成技术，包括输入[Khattab et al., 2022]、中间[Borgeaud et al., 2022]和输出层[Liang et al., 2023]。虽然“中间”和“输出层”更有效，但存在训练需求和低效率的问题。

RAG是一个通过整合外部知识库来增强LLMs的范式。它采用了一种协同方法，结合了信息检索机制和上下文学习（ICL）来增强LLM的性能。在这个框架中，通过搜索算法先去检索和用户提示语+query有关的信息。然后将这些信息编织到LLM的提示语中，为生成过程提供额外的上下文。RAG的主要优势在于它消除了为特定任务应用程序重新培训LLMs的需要。开发人员可以附加一个外部知识库，丰富输入，从而改进模型的输出精度。RAG由于其高实用性和低准入门槛，已经成为LLMs系统中最受欢迎的架构之一，许多会话产品几乎完全是在RAG上构建的。RAG工作流包括三个关键步骤。首先，将语料库划分为离散块，利用编码器模型在其上构建向量索引。其次，RAG根据它们与query和索引块的向量相似性来标识和检索块。最后，模型根据从检索块中收集的上下文信息生成回答。这些步骤构成了RAG流程的基本框架，支持其信息检索和上下文感知生成功能。接下来，我们将介绍RAG研究框架。

3 RAG框架

RAG研究范式不断演变，本节主要描述其进展。我们将其分类为三种类型：Naive RAG、Advanced RAG和Modular RAG。虽然RAG在成本效益上超过了原生LLM的性能，但也表现出了几个局限性。
Advanced RAG和Modular RAG的发展是为了解决Naive RAG中的特定缺陷。


3.1 Naive RAG
Naive RAG研究范式代表了最早的方法论，它在ChatGPT广泛采用后不久就获得了显著地位。Naive RAG遵循一个传统的流程，包括索引、检索和生成。它也被称为“检索-阅读”框架。

索引（Indexing）

索引过程是数据准备中的关键初始步骤，它在离线时进行，并涉及几个阶段。它从数据索引开始，原始数据被清洗和提取，各种文件格式（如PDF、HTML、Word和Markdown）被转换成标准化的纯文本。为了适应语言模型的上下文限制，这些文本随后被分割成更小、更易管理的块，这个过程被称为分块。这些块随后通过Embedding模型（在推理效率和模型大小之间取得了平衡）转换为向量。这有助于在检索阶段进行相似性比较。最后，创建一个索引来存储这些文本块及其向量嵌入作为键值对，这允许高效和可扩展的搜索能力。

检索（Retrieval）

在收到用户query后，系统使用与索引阶段相同的编码模型将输入转换为向量表示。然后，它计算query向量与索引语料库中向量化块之间的相似度分数。系统优先检索与query最相似的前K个块。这些块随后被用作解决用户请求的扩展上下文基础。

生成（Generation）

用户query和选定的文档被合成为一个连贯的提示，大语言模型来进行一个回应。模型的回答方式可能因任务特定标准而异，可以允许它利用其固有的参数知识，也可以将其回应限制在提供的文档信息内。在持续的对话中，任何现有的对话历史可以集成到提示中，使模型能够有效地进行多轮对话互动。

Naive RAG的缺点

Naive RAG在三个关键领域面临重大挑战:“检索”、“生成”和“增强”。

检索质量带来了各种各样的挑战，包括精度低，导致检索块不对齐以及潜在的问题，如幻觉或半空中掉落（mid-air drop）。低回忆率也会发生，导致无法检索所有相关的块，从而阻碍了LLMS全面回应的能力。过时的信息使问题进一步复杂化，可能产生不准确的检索结果。

回复生成质量有幻觉，其中模型生成的答案不基于所提供的上下文，以及模型输出中不相关的上下文和潜在的毒性或偏见问题。

增强过程的挑战在于如何在有效地将检索段落的上下文与当前生成任务集成，中间可能导致不连贯或不连贯的输出。冗余和重复也是一个问题，特别是当多个检索的段落包含相似的信息时，会导致生成的响应中出现重复的内容。

另一个挑战是如何识别多个检索段落对生成任务的重要性和相关性，需要适当平衡每个段落的价值。此外，协调不同的写作风格和语调，以确保输出的一致性是至关重要的。

最后，也存在生成模型过度依赖增强信息的风险，可能导致输出只是重复检索的内容，而没有提供新值或合成信息。

3.2 Advanced RAG
Advanced RAG是为了解决Naive RAG的局限性而开发的。在检索质量方面，Advanced RAG实现了预检索和后检索策略。为了解决Naive RAG在索引方面的挑战，Advanced RAG通过滑动窗口、细粒度分割和元数据等技术优化了索引方法。它还引入了各种方法来优化检索过程。

预检索过程（Pre-Retrieval Process）

优化数据索引：优化数据索引的目标是提高被索引内容的质量。这涉及到五个主要策略：增强数据粒度、优化索引结构、添加元数据、对齐优化和混合检索。

增强数据粒度旨在提高文本标准化、一致性、事实准确性和丰富上下文，以改善RAG系统的性能。这包括去除不相关信息，消除实体和术语的歧义，确认事实准确性，保持上下文，并更新过时的文档。

优化索引结构涉及调整块的大小以捕获相关上下文，跨多个索引路径进行查询，并利用图结构中的信息从节点之间的关系中捕获相关上下文。

添加元数据信息涉及将引用元数据（如日期和目的）集成到块中以进行过滤，并整合元数据（如引用的章节和子部分）以提高检索效率。

对齐优化通过在文档中引入“假设性query”来解决文档之间的对齐query和差异。


检索过程（Retrieval）

在检索阶段，主要关注的是通过计算query和块之间的相似度来识别合适的上下文。Embedding模型是这个过程的核心。在高级RAG中，有可能对Embedding模型进行优化。

微调嵌入。微调Embedding模型会显著影响RAG系统中检索内容的相关性。该过程包括自定义Embedding模型，以增强特定领域上下文中的检索相关性，特别是对于处理演化或罕见术语的专业领域。BGE Embedding模型（如BAAI2开发的BGE-large- en），就是一个可以微调以优化检索相关性的高性能Embedding模型的例子。可以使用GPT-3.5-turbo等语言模型生成用于微调的训练数据，以制定基于文档块的query，然后将其用作微调对。

动态嵌入。动态嵌入可适应单词使用的上下文，不像静态嵌入，它为每个单词使用单个向量[Karpukhin等，2020]。例如，在像BERT这样的变压器模型中，相同的单词可以根据周围的单词具有不同的嵌入。OpenAI的embeddingsada -02模型3建立在llm(如GPT)的原理之上，是一个复杂的动态Embedding模型，可以捕获上下文理解。然而，它可能不会像最新的全尺寸语言模型(如GPT-4)那样对上下文表现出同样的敏感性。

后检索过程（Post-Retrieval Process）

在从数据库检索到有价值的上下文后，将其与query作为输入合并到LLMs中时，解决上下文窗口限制带来的挑战至关重要。简单地一次性向LLMs展示所有相关文档可能会超过上下文窗口限制，会引入噪声，并阻碍对关键信息的关注。因此，对检索到的内容进行额外处理是必要的。

重新排名。重新排名检索到的信息，将最相关的内容重新定位到prompt的边缘，是关键策略。这个概念已经在LlamaIndex4、LangChain5和HayStack6等框架中实现。例如，Diversity Ranker6根据文档多样性优先重新排序，而LostInTheMiddleRanker则在提示的开头和结尾交替放置最佳文档。此外，cohereAI rerank7、bge-rerank8和LongLLMLingua9等方法重新计算相关文本和query之间的语义相似度，解决了基于向量模拟搜索的语义相似度解释的挑战。

提示压缩。研究表明，检索文档中的噪声对RAG性能有负面影响。在后处理中，重点是压缩不相关的上下文，突出关键段落，并减少整体上下文长度。Selective Context和LLMLingua10等方法利用小型语言模型来计算提示的互信息或困惑度，从而估计模块的重要性。Recomp11通过在不同粒度上训练压缩器来解决这个问题，而Long Context12和“Walking in the Memory Maze”13则设计了总结技术，以增强LLM在处理大量上下文时的关键信息感知。

3.3 Modular RAG
Modular RAG结构与传统的Naive RAG框架不同，提供了更大的多样性和灵活性。它整合了各种方法来增强功能模块，例如，整合一个搜索模块进行相似性检索，并在检索器中应用微调方法。重新结构化的RAG模块和迭代方法已经被开发出来以解决特定问题。Modular RAG范式在RAG领域越来越成为常态，允许在多个模块之间进行串行工作流或端到端训练。图3展示了三种RAG范式的比较。然而，Modular RAG并不是独立的。Advanced RAG是Modular RAG的一种专门形式，而Naive RAG本身是Advanced RAG的一个特例。这三种范式之间的关系是继承和发展的关系。


图3 RAG三种范式的比较

新模块

搜索模块（Search Module）。与Naive/Advanced RAG中的相似性检索不同，搜索模块针对特定场景进行了定制，并在额外的语料库上进行直接搜索。这种整合是通过LLM生成的代码实现的，查询语言用的是如SQL或Cypher以及其他定制工具。这些搜索的数据源可以包括搜索引擎、文本数据、表格数据和知识图谱[Wang et al., 2023d]。

记忆模块（Memory Module）。这个模块利用LLM的记忆能力来指导检索。这种方法涉及识别与当前输入最相似的记忆。Selfmem [Cheng et al., 2023b]利用检索增强的生成器迭代地创建一个无界记忆池，结合“原始query”和“双重query”。通过使用检索增强生成模型（使用模型本身的输出来改进自己的模型），可以使文本在推理过程中与数据分布更加一致。因此，利用的是模型的输出被用作训练数据[Wang et al., 2022a]。

融合（Fusion）。RAG-Fusion [Raudaschl, 2023]通过多查询方法增强了传统搜索系统，该方法将用户query扩展为多个query，解决了用户query的局限性。这种多query方法通过扩展用户query，将答案与多个query对齐，从而提高了信息检索的效率和质量。

路由（Routing）。RAG系统的检索过程利用了不同领域、语言和格式的多样化来源，可以根据情况交替或合并[Li et al., 2023b]。query路由器决定用户query的后续操作，选项包括从摘要、搜索特定数据库或将不同路径合并为单个响应。query路由器还会选择query的适当数据存储，可能包括向量存储、图数据库或关系数据库，或多文档存储的索引层次结构，例如，摘要索引和文档块向量索引。query路由器的决策是预定义的，并通过LLM调用执行，将query定向到所选索引。

预测（Predict）。它解决了检索内容中的冗余和噪声问题。与直接从数据源检索不同，这个模块利用LLM生成必要的上下文[Yu et al., 2022]。由LLM生成的内容比通过直接检索获得的内容更有可能包含相关信息。

任务适配器（Task Adapter）。这个模块专注于将RAG适应于各种下游任务。UPRISE自动化了从预构建的数据池中检索零样本任务输入的提示，从而增强了跨任务和模型的通用性[Cheng et al., 2023a]。同时，PROMPTAGA-TOR [Dai et al., 2022]利用LLM作为少量样本query生成器，并根据生成的数据创建任务特定的检索器。通过利用LLM的泛化能力，它能够在最少的示例下开发出特定于任务的端到端检索器。

新模式

Modular RAG的组织结构具有高度的适应性，允许在RAG过程中替换或重新排列模块以适应特定query的上下文。

Naive RAG和Advanced RAG都可以认为是由一些固定的模块组成的。如图3所示，Naive RAG主要由“Retrieve”和“Read”模块组成。Advanced RAG的模式是在Naive RAG的基础上，添加了“Rewrite”和“Rerank”模块。但总体而言，Modular RAG具有更大的多样性和灵活性。

目前的研究主要探讨了两种组织范式。前者涉及添加或替换模块，而后者侧重于调整模块之间的组织流程。这种灵活性使RAG过程能够有效地处理各种任务。

增加或更换模块。引入或替换模块的策略包括维护检索-读取过程的核心结构，同时集成其他模块以增强特定功能。RRR模型[Ma等，2023a]引入了重写-检索-读取过程，利用LLM性能作为重写模块的强化学习激励。这使重写器能够微调检索问题，从而提高读取器的下游任务性能。

类似地，在Generate-Read [Yu et al.， 2022]等方法中，模块可以选择性地交换，其中LLM的生成模块取代了检索模块。背诵-阅读（Recite-Read）方法[Sun et al.， 2022]将外部检索转换为基于模型权重的检索，要求LLM首先记住特定于任务的信息，随后产生能够处理知识密集型自然语言处理任务的输出。

调整模块间的流程。在模块流调整领域，重点是加强语言模型和检索模型之间的交互。DSP [Khattab等，2022]引入了演示-搜索-预测框架，将上下文学习系统视为一个明确的程序，而不是最终的任务提示，从而更有效地处理知识密集型任务。ITER-RETGEN [Shao等，2023]方法利用生成的内容来指导检索，在检索-读取-检索-读取流程中迭代地实现“检索增强生成”和“生成增强检索”。这种方法展示了一种使用一个模块的输出来改进另一个模块的功能的创新方法。

优化RAG流程

RAG流程的优化旨在提高RAG系统中信息的效率和质量。当前的研究集中在整合多样化的搜索技术，完善检索步骤，融入认知回溯，实施灵活的query策略，并利用嵌入相似性。这些努力共同致力于在RAG系统中实现检索效率和上下文信息深度之间的平衡。

混合搜索探索。RAG系统通过智能整合各种技术，包括基于关键词的搜索、语义搜索和向量搜索，来优化其性能。这种方法利用每种方法的独特优势，以适应多样化的query类型和信息需求，确保检索到高度相关和内容丰富的信息。

递归检索和问题引擎。递归检索涉及在初始检索阶段获取较小的块以捕获关键语义意义。随后，在过程的后期阶段，向LLM提供包含更多上下文信息的较大块。这种两步检索方法有助于平衡响应效率和提供丰富内容。

StepBack-prompt。该方法鼓励LLM远离具体实例，围绕更广泛的概念和原则进行推理[Zheng et al., 2023]。实验结果表明，当使用向后提示时，各种具有挑战性的基于推理的任务的性能显著提高，突显了它们对RAG过程的自然适应性。这些检索增强步骤可以应用于生成向后提示的响应以及最终的问答过程中。

子query。根据场景，可以采用各种query策略，例如使用LlamaIndex提供的query引擎，利用树query，使用向量query，或执行简单的顺序query块。

假设文档嵌入（Hypothetical Document Embeddings，HyDE）。HyDE基于这样一个信念：生成的答案可能比直接查询在嵌入空间中更接近。使用LLM，HyDE为query创建一个假设文档（答案），嵌入这个文档，并使用结果嵌入来检索与假设文档相似的真实文档。这种方法不是基于query的嵌入相似性，而是关注从一个答案到另一个答案的嵌入相似性[Gao et al., 2022]。然而，它可能不会始终产生理想的结果，特别是在语言模型不熟悉主题内容时，可能导致更多错误实例。

4.检索
在RAG的背景下，从数据源高效检索相关文档至关重要。然而，创建一个熟练的检索器面临着重大挑战。本节将介绍三个基本问题：1）如何实现准确的语义表示？2）哪些方法可以对齐query和文档？3）如何将检索器的输出与大语言模型（LLM）的偏好对齐？

4.1 构建准确的语义空间
在RAG中，语义空间对于将query和文档映射到多维空间至关重要。在这个语义空间中的检索准确性将会显著影响RAG的结果。本节将介绍两种构建准确语义空间的方法。

块优化（Chunk optimization）

在处理外部文档时，最初的步骤涉及将它们分解成较小的块以提取细粒度特征，然后嵌入这些特征以表示它们的语义。然而，嵌入过大或过小的文本块可能会导致较次的优化结果。因此，确定语料库中文档的最佳块大小对于确保检索结果的准确性和相关性至关重要。

选择合适的块策略需要仔细考虑几个重要因素，如索引内容的性质、Embedding模型及其最佳块大小、用户query的预期长度和复杂性，以及应用程序对检索结果的具体应用（例如，语义搜索或问答）。例如，选择块模型应基于内容的长度。此外，不同的Embedding模型在不同的块大小下表现出不同的性能特征。例如，sentence-transformer在单个句子上表现更好，而text-embedding-ada-002在包含256-512个token的块中表现出色。

此外，用户输入query的长度和复杂性以及应用程序对检索结果的具体需求（例如，语义搜索或问答）也会影响块策略的选择。实际上，获取精确的查询结果会涉及灵活应用不同的块策略。没有一种“最佳”策略适用于所有情况，只有最适合特定上下文的策略。

当前的RAG研究探索了各种块优化技术，旨在提高检索效率和准确性。一种方法为使用滑动窗口技术，通过合并多个检索过程中的全局相关信息来实现分层检索。另一种策略，称为“small2big”方法，使用小文本块进行初始搜索阶段，随后为语言模型提供更大的相关文本块进行处理。

抽象嵌入技术根据文档摘要（或摘要）优先考虑基于文档摘要的前K个检索，提供对整个文档内容的全面理解。此外，元数据过滤技术利用文档元数据来增强过滤过程。图索引技术（一种创新的方法）将实体和关系转换为节点和连接，显著提高了相关性，这种相关性在多跳问题的背景下表现得尤为明显。

微调Embedding模型

一旦确定了合适的块大小，下一步关键步骤是使用Embedding模型将这些块和query嵌入到语义空间中。嵌入的有效性对于模型表示语料库的能力至关重要。

最近的研究引入了显著的Embedding模型，如AngIE、Voyage、BGE等。这些模型已经在广泛的语料库上进行了预训练。然而，当应用于特定领域时，它们准确捕捉领域特定信息的能力可能受到限制。

此外，Embedding模型的特定任务微调对于确保模型从内容相关性方面理解用户问题至关重要。没有微调的模型可能无法充分满足特定任务的需求。因此，对Embedding模型进行微调对于下游应用程序至关重要。在嵌入微调方法中有两种主要的范式。

领域知识微调。为了确保Embedding模型准确地捕获特定于领域的信息，必须利用特定于领域的数据集进行微调。这个过程与标准语言模型微调不同，主要在于所涉及的数据集的性质。用于Embedding模型微调的数据集通常包含三个主要元素:问题（query）、语料库和相关文档。该模型使用这些query来识别语料库中的相关文档。然后，根据响应query检索这些相关文档的能力来衡量模型的有效性。数据集构建、模型微调和评估阶段各有不同的挑战。LlamaIndex [Liu, 2023]引入了一套关键的类和函数，旨在增强Embedding模型微调工作流程，从而简化这些复杂的过程。通过管理充满领域知识的语料库并利用所提供的方法，可以熟练地微调Embedding模型，使其与目标领域的特定需求紧密结合。

为下游任务微调。在利用RAG进行这些任务时，出现了创新的方法，利用LLM的能力来微调Embedding模型。例如，PROMPTAGATOR利用LLM作为少量样本query生成器来创建任务特定的检索器，解决了监督微调中的挑战，特别是在数据稀缺的领域。另一种方法，LLM-Embedder，利用LLM生成跨多个下游任务的数据奖励信号。检索器使用两种类型的监督信号进行微调：数据集的硬标签和LLM的软奖励。这种双信号方法促进了更有效的微调过程，使Embedding模型适应多样化的下游应用。

虽然这些方法通过整合领域知识和任务特定微调提高了语义表示，但检索器可能并不总是与某些LLMs兼容。为了解决这个问题，一些研究人员已经探索了使用LLMs的反馈直接监督微调过程。这种直接监督旨在更紧密地将检索器与LLM对齐，从而提高下游任务的性能。关于这个话题的更全面讨论将在第4.3节中介绍。

4.2 对齐query和文档
在RAG应用程序的上下文中，检索器可以使用单个Embedding模型对query和文档进行编码，或者为每个模型使用单独的模型。此外，用户的原始query可能会受到措辞不精确和缺乏语义信息的影响。因此，将用户query的语义空间与文档的语义空间保持一致是至关重要的。本节将介绍两种旨在实现这种对齐的基本技术。

重写query

重写query是将query和文档的语义对齐的基本方法。例如，Query2Doc和ITER-RETGEN利用LLMs创建一个伪文档，通过结合原始query和额外的指导来实现。HyDE通过使用文本线索构建query向量，生成一个“假设”文档来捕捉关键模式。RRR引入了一个框架，反转了传统的检索和阅读顺序，专注于重写query。STEP-BACKPROMPTING使LLMs能够基于高级概念执行抽象推理和检索。此外，多query检索方法利用LLMs同时生成和执行多个搜索query，对于解决具有多个子query的复杂query具有优势。

嵌入转换

除了像重写query的策略外，还有专门为嵌入转换设计的更细粒度的技术。LlamaIndex [Liu, 2023]通过引入一个可以集成在query编码器之后的适配器模块来举例说明这一点。这个适配器有助于微调，从而优化query嵌入的表示，将它们映射到与预期任务更紧密结合的潜在空间。

SANTA [Li et al., 2023d] 方法解决了query与结构化外部文档对齐的query，尤其是在处理结构化数据与非结构化数据之间的不匹配时。这种方法通过两种预训练策略提高了检索器识别结构化信息的能力：首先，利用结构化数据与非结构化数据之间的固有对齐关系，引导结构化意识预训练方案中的对比学习；其次，采用遮蔽实体预测。后者采用以实体为中心的遮蔽策略，激励语言模型预测和填补遮蔽的实体，从而加深对结构化数据的理解。

处理结构化外部文档，特别是处理结构化和非结构化数据之间的差异时，SANTA方法通过两种预训练策略提高了检索器识别结构化信息的能力：首先，利用结构化和非结构化数据之间的固有对齐来指导结构感知的对比学习；其次，通过实体中心掩蔽策略实施掩蔽实体预测。后者使用实体中心掩蔽策略，促使语言模型预测和完成掩蔽实体，从而促进对结构化数据更深刻的理解。

4.3 对齐检索器和LLM
在RAG工作流中，通过各种技术提高检索命中率不一定会改善最终结果，因为检索的文档可能与llm的特定需求不一致。因此，本节将介绍两种方法，旨在使检索器输出与llm的首选项保持一致。

微调检索器

一些研究利用LLMs的反馈信号来优化检索模型。例如，AAR利用编码器-解码器架构为预训练检索器引入监督信号。这是通过识别LM的首选文档通过FiD交叉注意力分数来实现的。随后，检索器通过硬负采样和标准的交叉熵损失进行微调。最终，经过优化的检索器可以直接应用于增强目标LMs，在目标任务中实现更好的性能。此外，建议LLMs可能更喜欢关注可读性而非信息丰富的文档。

REPLUG利用检索器和LLM来计算检索文档的概率分布，然后通过计算KL散度进行监督训练。这种简单而有效的训练方法通过使用LM作为监督信号来增强检索模型的性能，消除了特定交叉注意力机制的需求。UPRISE也利用冻结的LLMs来微调提示检索器。LLM和检索器都以提示-输入对作为输入，并利用LLM提供的分数来监督检索器的训练，有效地将LLM视为数据集标签器。此外，Atlas提出了四种监督微调Embedding模型的方法：

注意力蒸馏。这种方法利用LLM在输出时生成的交叉注意力分数来蒸馏模型的知识。
EMDR2。通过使用期望最大化算法，这种方法使用检索到的文档作为潜在变量进行训练。
困惑度蒸馏。直接使用生成标记的困惑度作为指标进行训练。
LOOP。这种方法提出了一种基于文档删除对LLM预测影响的新损失函数，提供了一种高效的训练策略，以更好地适应特定任务。
这些方法旨在改善检索器和LLM之间的协同作用，从而提高检索性能和对用户查询的更准确响应。

适配器

微调模型可能会面临一些挑战，例如通过API集成功能或解决由于本地计算资源有限而产生的约束。因此，一些方法选择整合外部适配器来帮助对齐。

PRCA通过上下文提取阶段和奖励驱动阶段来训练适配器。然后，使用基于Tokens的自回归策略优化检索器的输出。Tokens过滤方法利用交叉注意力分数高效地过滤Tokens，只选择得分最高的输入Tokens。RECOMP引入了提取性和生成性压缩器用于摘要生成。这些压缩器要么选择相关句子，要么合成文档信息，创建针对多文档查询量身定制的摘要。

此外，PKG通过指令式微调引入了一种创新的方法，将知识集成到白盒模型中。在这种方法中，检索器模块被直接替换为根据query生成相关文档。这种方法有助于解决微调过程中遇到的困难，并提高模型性能。

5 生成器（Generation）
RAG的一个关键组成部分是生成器，它负责将检索到的信息转换为连贯流畅的文本。与传统的语言模型不同，RAG生成器通过整合检索到的数据来提高准确性和相关性。在RAG中，生成器的输入不仅包括典型的上下文信息，还包括检索器获取的相关文本片段。这种全面的输入使生成器能够深刻理解query的上下文，从而产生更具信息性和上下文相关性的回答。

此外，生成器受到检索文本的指导，以确保生成的内容与获取的信息保持一致。多样化的输入数据导致了生成阶段的重点在于完善大模型对来自问题和文档的输入数据的适应。在以下子节中，我们将通过深入探讨后检索处理和微调方面来介绍生成器。

5.1 不可微调模型的后检索（Post-retrieval with Frozen LLM）
在不可调的LLM领域，许多研究依赖于像GPT-4这样的成熟模型，利用它们的全面内部知识系统地从各种文档中合成检索到的信息。然而，这些大模型仍然存在一些挑战，包括上下文长度的限制和对冗余信息的敏感性。为了解决这些问题，某些研究工作已经将重点转向了后检索处理。

后检索处理涉及处理、过滤或优化检索器从大型文档数据库中检索到的相关信息。其主要目标是提高检索结果的质量，使其更紧密地与用户需求或后续任务对齐。它可以被视为对检索阶段获得的文档进行重新处理。常见的后检索处理操作通常包括信息压缩和重新排序。

信息压缩

检索器擅长从庞大的知识库中检索相关信息，但管理检索文档中的大量信息是一个挑战。正在进行的研究旨在扩展大语言模型的上下文长度来解决这个问题。然而，当前的大模型仍然在上下文限制方面存在困难。因此，在某些情况下，信息压缩变得必要。信息压缩对于减少噪声、解决上下文长度限制和增强生成效果至关重要。

PRCA通过训练一个信息提取器来解决这个问题。在上下文提取阶段，当给定输入文本Sinput时，它能够产生一个输出序列Cextracted，代表输入文档的压缩上下文。训练过程旨在最小化Cextracted和实际上下文Ctruth之间的差异。

同样，RECOMP采用相似的方法，通过对比学习训练一个信息压缩器。每个训练数据点由一个正样本和五个负样本组成，编码器在整个过程中使用对比损失进行训练。

另一个研究采取了不同的方法，旨在通过减少文档数量来提高模型答案的准确性。在Ma等的研究中，他们提出了“Filter-Reranker”范式，结合了LLMs和小型语言模型（SLMs）的优势。在这个范式中，SLMs充当过滤器，而LLMs作为重新排序代理。研究表明，指导LLMs重新排列SLMs识别的挑战性样本可以显著提高各种信息提取（IE）任务的性能。

重新排序（Reranking）

重新排序模型是优化从检索器检索到的文档集的关键。当引入额外的上下文时，语言模型经常面临性能下降的问题，重新排序可以有效地解决这个问题。核心概念包括重新排列文档记录，将最相关的项放在最上面，从而限制文档的总数。这既解决了检索过程中上下文窗口展开的难题，又提高了检索效率和响应速度。重新排序模型在整个信息检索过程中扮演双重角色，既充当优化器，又充当精炼器。它为后续的语言模型处理提供了更有效和准确的输入[Zhuang等，2023]。

上下文压缩被整合到重新排序过程中，以提供更精确的检索信息。这种方法需要减少单个文档的内容并过滤整个文档，其最终目标是在搜索结果中显示最相关的信息，以便更集中、更准确地显示相关内容。

5.2 微调LLM以适应RAG
优化RAG模型中的生成器是一个关键方面。生成器的角色是将检索到的信息转化为相关文本，形成模型的最终输出。优化生成器的目的是确保生成的文本既自然又有效地利用检索到的文档，以更好地满足用户的查询需求。

在标准的LLM生成任务中，输入通常由query组成。RAG通过将query和检索器获取的各种结构化/非结构化文档整合到输入中，从而脱颖而出。这种额外的信息可以显著影响模型的理解，尤其是对于较小的模型。在这种情况下，将模型微调以适应query和检索到的文档的输入变得至关重要。在将输入呈现给微调模型之前，通常会对检索器检索到的文档进行后检索处理。值得注意的是，RAG中生成器的微调方法与LLMs的一般微调方法相一致。接下来，我们将简要描述一些代表性的工作，涉及数据（格式化/非格式化）和优化函数。

一般优化过程

作为一般优化过程的一部分，训练数据通常由输入-输出对组成，旨在训练模型在给定输入x的情况下产生输出y。在Self-Mem的工作[Cheng et al., 2023b]中，采用了传统的训练过程，其中给定输入x，检索相关文档z（在论文中选择Top-1），然后通过整合（x, z），模型生成输出y。该论文利用了两种常见的微调范式，即联合编码器和双编码器[Arora et al., 2023, Wang et al., 2022b, Lewis et al., 2020, Xia et al., 2019, Cai et al., 2021, Cheng et al., 2022]。

在联合编码器范式中，使用基于编码器-解码器的标准模型。这里，编码器首先对输入进行编码，然后通过注意力机制，解码器将编码结果结合起来，以自回归的方式生成标记。另一方面，在双编码器范式中，系统设置了两个独立的编码器，每个编码器分别编码输入（query，上下文）和文档。然后，解码器通过序列处理双向交叉注意力处理输出。这两种架构都使用Transformer作为基础块，并使用负对数似然损失进行优化。

利用对比学习

在为语言模型准备训练数据时，通常会创建输入和输出的交互对。这种传统方法可能导致“曝光偏差”，模型只训练在单个正确的输出示例上，从而限制了模型对可能输出范围的曝光。这种限制可能会通过使模型过度拟合训练集中的特定示例，从而降低模型在现实世界中的性能，因为它减少了模型在各种上下文中的泛化能力。

为了缓解曝光偏差，SURGE提出了使用图-文本对比学习。这种方法包括一个对比学习目标，促使模型产生一系列合理且连贯的响应，超越了训练数据中遇到的实例。这种方法对于减少过拟合并加强模型在各种上下文中的泛化能力至关重要。

对于处理结构化数据的检索任务，SANTA框架实现了一个三部分的训练方案，有效封装了结构和语义细节。初始阶段专注于检索器，其中对比学习被用来细化问题和文档嵌入。

随后，生成器的初步训练阶段利用对比学习将结构化数据与其非结构化文档描述对齐。在生成器训练的进一步阶段，模型认识到实体语义在文本数据表示学习中的关键作用（Sciavolino等强调）。这个过程从识别结构化数据中的实体开始，然后在生成器的输入数据中对这些实体应用掩码，为模型预测这些掩码元素铺平道路。

训练过程随着模型学习利用上下文信息重建被遮蔽的实体而进展。这个练习培养了模型对文本数据结构语义的理解，并促进了结构化数据中相关实体的对齐。总体优化目标是训练语言模型准确地恢复被遮蔽的跨度，从而丰富了模型对实体语义的理解。

6 增强器
RAG是一个知识密集型的努力，它在语言模型训练的预训练、微调和推理阶段融合了各种技术方法。增强阶段、增强数据的来源和增强过程是阐明RAG发展关键技术的三个方面。这些方面阐明了RAG核心组件的分类，如图4所示。


图4:RAG核心组件的分类
6.1 增强阶段的RAG
RAG是一项知识密集型的工作，在语言模型训练的预训练、微调和推理阶段整合了各种技术方法。

预训练阶段（Pre-training）

在预训练阶段，研究人员研究了通过基于检索的策略来支持开放域QA的ptm的方法。REALM模型采用一种结构化、可解释的知识嵌入方法，将预训练和微调框架为掩码语言模型（MLM）框架内的检索然后预测工作流[Arora等，2023]。

RETRO [Borgeaud等，2022]利用检索增强技术从头开始进行大规模预训练，实现了模型参数的减少，同时在困惑度方面超过了标准的GPT模型。RETRO的独特之处在于，它有一个额外的编码器，用于处理从外部知识库检索到的实体的特征，它建立在GPT模型的基础结构上。

Atlas[Izacard等，2022]还在预训练和微调阶段将检索机制纳入T5架构[rafael等，2020]。它使用预训练的T5来初始化编码器-解码器语言模型，使用预训练的Contriever来初始化密集检索器，提高了复杂语言建模任务的效率。

此外，COG[Lan et al., 2022]引入了一种新颖的文本生成方法，模仿从现有集合中复制文本片段。利用高效的向量搜索工具，COG计算并索引文本片段的上下文有意义表示，在问答和领域适应等领域表现出优于RETRO的性能。随着缩放定律的出现，模型参数的增长推动了自回归模型成为主流。研究人员正在将RAG方法扩展到更大的预训练模型，RETRO++就是这一趋势的典范，它在保持或提高性能的同时，扩大了模型参数[Wang et al., 2023b]。

实证证据强调了在文本生成质量、事实准确性、减少毒性和下游任务熟练度方面的显著改进，特别是在知识密集型应用如开放域问答中。这些结果意味着将检索机制整合到基于预训练的策略中，是构建强大基础模型的一个有前途的途径，这些模型在使用较少参数的同时，表现上在困惑度、文本生成质量和任务特定性能方面都优于标准GPT模型。这种方法特别擅长处理知识密集型任务，并通过在专业语料库上进行训练来促进领域特定模型的开发。

虽然面临一些挑战（如需要大量预训练数据集和资源/随着模型大小增加更新频率会降低），但这种方法具备使模型有更强的韧性的显著优势。一旦训练完成，增强检索模型可以在不依赖外部库的情况下独立运行，提高了生成速度和操作效率。这些潜在的收益使这种方法成为人工智能和机器学习中持续研究和创新的一个引人注目的主题。

微调阶段（Fine-tuning Stage）

微调阶段是RAG模型发展的关键部分，它允许模型适应特定任务的需求。在这个阶段，RAG模型通过微调来增强其性能，这涉及到对检索器和生成器的优化。微调的主要目标是提高模型在特定任务上的表现，这通常通过调整模型的参数来实现，以便更好地理解和生成与任务相关的文本。

在微调检索器方面，研究者们专注于提高检索内容的语义表示质量。这通常通过直接微调Embedding模型来实现，该模型负责将query和文档转换为语义空间中的向量表示。微调过程确保检索器能够更准确地捕捉到与用户query相关的信息，从而提高检索结果的相关性和准确性。

微调生成器则关注于生成器如何将检索到的信息整合到连贯、流畅的文本中。这涉及到对生成器进行微调，使其能够根据检索到的上下文生成更准确、更相关的回答。微调还可以帮助模型适应不同的输入数据格式，例如知识图谱、文本对等，以及生成特定格式的内容，如指令数据集。

此外，微调还可以用于调整检索器和生成器之间的协同作用，以提高模型的整体性能。这种协同微调有助于模型在处理复杂任务时，如多步推理问题，能够更有效地利用检索到的信息。

推理阶段（Inference Stage）

推理阶段是RAG模型实际应用中的关键环节，它涉及到模型如何将检索到的信息与生成过程相结合，以产生最终的输出。在这个阶段，RAG模型需要与大语言模型（LLMs）紧密集成，以实现高效的信息检索和文本生成。

为了克服传统RAG方法（Naive RAG）的局限性，研究者们开发了更先进的技术，这些技术在推理阶段引入了更丰富的上下文信息。例如，DSP框架通过在不可微调的LLMs和检索模型（RMs）之间进行复杂的自然语言文本交换，增强了上下文信息，从而提高了生成文本的质量。

其他方法，如PKG，为LLMs提供了一个知识引导模块，使得模型能够在不修改LMs参数的情况下检索相关信息，这使得模型能够执行更复杂的任务。CREA-ICL和RE-CITE方法则通过跨语言知识的同步检索和直接从LLMs中采样段落来增强上下文。

对于需要多步推理的任务，如ITRG和IRCoT，它们采用了迭代检索和推理策略，以逐步构建和完善答案。这些方法通过交替进行检索和生成过程，提高了模型在处理复杂问题时的适应性和准确性。

总的来说，推理阶段的这些增强技术提供了一种轻量级、成本效益高的方法，利用预训练模型的能力，同时不需要进一步的训练。这种方法的主要优势在于能够在保持LLM参数不变的情况下，根据特定任务需求提供相关的上下文信息。然而，这种方法也有其局限性，包括对数据处理和优化的精细要求，以及对基础模型能力的依赖。为了有效应对多样化的任务需求，这种方法通常需要与程序化优化技术相结合，如逐步推理、迭代检索和自适应检索策略。

6.2 增强数据来源
RAG模型的有效性在很大程度上受到增强数据来源选择的影响。不同的知识层次和维度需要不同的处理技术。它们被归类为非结构化数据、结构化数据和由LLMs生成的内容。图5展示了具有不同增强方面的代表性RAG研究的技术树。叶子以三种不同的颜色表示，代表使用各种类型的数据进行增强：非结构化数据、结构化数据和由LLMs生成的内容。图表清楚地显示，最初，增强主要通过非结构化数据（如纯文本）实现，这种方法后来扩展到包括使用结构化数据（例如知识图谱）以进一步改进。最近，研究趋势越来越倾向于利用LLMs自身生成的内容进行检索和增强。


增强非结构化数据（Augmented with Unstructured Data）

非结构化文本是从语料库中收集的，例如用于微调大模型的提示数据[Cheng et al., 2023a]和跨语言数据[Li et al., 2023b]。检索单元从token（例如kNN-LM[Khandelwal et al., 2019]）到短语（例如NPM，COG[Lee et al., 2020, Lan et al., 2022]）和文档段落，更细粒度的检索可以提升检索精度，但这带来的代价是会增加检索复杂性。FLARE[Jiang et al., 2023b]引入了一种主动检索方法，由LM生成低概率词触发。它创建一个临时句子用于文档检索，然后使用检索到的上下文重新生成句子以预测后续句子。RETRO使用前一个块来检索块级别的最近邻居，结合前一个块的上下文，引导生成下一个块。为了保持因果关系，下一个块Ci的生成只使用前一个块N(Ci−1)的最近邻居，而不是N(Ci)。

增强结构化数据（Augmented with structured Data）

结构化数据，如知识图谱（KGs），提供高质量的上下文并减轻模型幻觉。RET-LLMs[Modarressi et al., 2023]从过去的对话中构建知识图谱记忆以供未来参考。SUGRE[Kang et al., 2023]利用图神经网络（GNNs）编码相关的KG子图，通过多模态对比学习确保检索到的事实与生成的文本保持一致。KnowledGPT[Wang et al.， 2023]生成知识库搜索query，并将知识存储在个性化库中，增强了RAG模型的知识丰富度和上下文性。增强非结构化数据

llms生成内容在RAG中（LLMs-Generated Content in RAG）

针对RAG中外部辅助信息的局限性，一些研究侧重于利用LLMs的内部知识。SKR [Wang等，2023e]将query分类为已知或未知，有选择地应用检索增强。GenRead [Yu et al.， 2022]用LLM生成器替换了检索器，发现LLM生成的上下文通常包含更准确的答案，因为它更好地符合因果语言建模的预训练目标。Selfmem [Cheng et al.， 2023b]使用检索增强生成器迭代创建无界内存池，使用内存选择器选择作为原始query对偶query的输出，从而自增强生成模型。

这些方法强调了RAG中创新数据源利用的广度，努力提高模型性能和任务有效性。

6.3 增强过程
在RAG领域，标准的做法通常涉及单一的检索步骤，然后进行生成，这可能导致效率低下。一个值得注意的问题是“中间丢失（Lost in the middle）”现象，当单一检索产生冗余内容时，可能会稀释或矛盾关键信息，从而降低生成质量[Liu et al., 2023a]。此外，这种单一检索对于需要多步推理的复杂问题通常是不够的，它提供的信息范围有限[Yoran et al., 2023]。

为了规避这些挑战，当代研究提出了优化检索过程的方法：迭代检索、递归检索和自适应检索。迭代检索允许模型基于初始query和迄今为止生成的文本多次收集文档，为LLMs提供更全面的知识库[Borgeaud et al., 2022, Arora et al., 2023]。这种方法已被证明通过多次检索迭代增强了后续答案生成的鲁棒性。然而，它可能会受到语义不连续性和无关信息积累的影响，因为迭代检索可能会引入与任务无关的信息。

迭代检索（Iterative Retrieval）

迭代检索在RAG模型中是一个过程，其中文档基于初始query和迄今为止生成的文本多次收集，为LLMs提供更全面的知识库[Borgeaud et al., 2022, Arora et al., 2023]。这种方法已被证明通过多次检索迭代增强了后续答案生成的鲁棒性。然而，它可能会受到语义不连续和不相关信息积累的影响，因为它通常依赖于n个Tokens序列来划定生成文本和检索文档之间的边界。为了解决特定的数据场景，使用了递归检索和多跳检索技术。递归检索涉及到以分层方式处理和检索数据的结构化索引，其中可能包括在基于该摘要执行检索之前对文档或冗长PDF的各个部分进行总结。随后，文档中的二次检索细化了搜索，体现了该过程的递归性质。相比之下，多跳检索旨在更深入地挖掘图结构数据源，提取相互关联的信息[Li et al.， 2023c]。

此外，一些方法集成了检索和生成的步骤。ITER-RETGEN [Shao等人，2023]采用协同方法，利用“检索增强生成”和“生成增强检索”来完成需要复制特定信息的任务。该模型利用处理输入任务所需的内容作为检索相关知识的上下文基础，这反过来又有助于在随后的迭代中生成改进的响应。

递归检索（Recursive Retrieval）

该过程涉及根据从先前的搜索中获得的结果来不断细化搜索查询。递归检索的目标是通过反馈循环逐步收敛到最相关的信息，从而增强搜索体验。IRCoT [Trivedi等人，2022]使用思维链来指导检索过程，并使用获得的检索结果来细化CoT。ToC [Kim等人，2023]创建了一个澄清树，该树系统地优化了查询中的模糊部分。在用户需求从一开始就不完全明确，或者所寻求的信息高度专业或细微的情况下，它可能特别有用。该过程的递归性质允许不断学习和适应用户的需求，通常导致对搜索结果的满意度提高。

适应性检索（Adaptive Retrieval）

Adaptive检索，如Flare和Self-RAG[Jiang et al., 2023b, Asai et al., 2023]，通过使LLMs主动确定检索的最佳时机和内容，从而提高RAG框架的效率和相关性。

这些方法属于更广泛的趋势，即LLMs在其操作中采用主动判断，如AutoGPT、Toolformer和Graph-Toolformer[Yang et al., 2023c, Schick et al., 2023]。例如，Graph-Toolformer将其检索过程划分为不同的步骤，其中llm主动使用检索器，应用Self-Ask技术，并使用少量提示来启动搜索查询。这种主动的姿态允许llm决定何时搜索必要的信息，类似于代理如何利用工具。

WebGPT [Nakano等人，2021]集成了一个强化学习框架，在文本生成过程中使用搜索引擎自主训练GPT-3模型。它使用特殊的tokens来引导这个过程，这些tokens促进了搜索引擎查询、浏览结果和引用引用等操作，从而通过使用外部搜索引擎扩展了GPT-3的功能。Flare通过监测生成过程的置信度来自动获取时序，如生成项的概率所示[Jiang等，2023b]。当概率低于某一阈值时，将激活检索系统收集相关信息，从而优化检索周期。

Self-RAG [Asai等人，2023]引入了“反射Tokens”，允许模型自省其输出。这些标记有两种:“检索”和“批评”。模型自主地决定何时激活检索，或者，预定义的阈值可能触发该流程。在检索过程中，生成器跨多个段落进行片段级波束搜索，以获得最连贯的序列。评论家分数用于更新细分分数，在推理过程中可以灵活地调整这些权重，从而调整模型的行为。Self-RAG的设计不需要额外的分类器或依赖于自然语言推理(NLI)模型，从而简化了何时使用检索机制的决策过程，并提高了模型在生成准确响应方面的自主判断能力。

LLM优化由于其日益普及而受到了极大的关注。诸如提示工程、微调(FT)和RAG等技术各有不同的特征，如图6所示。虽然快速工程利用了模型的固有功能，但优化llm通常需要同时应用RAG和FT方法。RAG和FT之间的选择应该基于场景的特定需求和每种方法的固有属性。表1给出了RAG和FT的详细比较。


6.4 RAG vs 微调
RAG就像给模型提供了一本教科书，用于定制信息检索，非常适合特定query。另一方面，《金融时报》就像一个学生，随着时间的推移将知识内化，更适合复制特定的结构、风格或格式。FT可以通过强化基础模型知识、调整输出和教授复杂指令来提高模型性能和效率。然而，它不适合集成新知识或快速迭代新用例。这两种方法，RAG和FT，并不是相互排斥的，而是可以互补的，可以在不同层次上增强模型的能力。在某些情况下，它们的组合使用可能产生最佳性能。涉及RAG和FT的优化过程可能需要多次迭代才能获得满意的结果。


7 RAG 评估
RAG（Retrieval-Augmented Generation）模型在自然语言处理（NLP）领域的快速发展和广泛应用，推动了对RAG模型性能评估的研究。评估RAG模型的主要目标是理解和优化这些模型在多样化应用场景中的性能。历史上，RAG模型的评估主要集中在它们在特定下游任务中的执行情况。这些评估通常采用适合任务的既定指标，例如，对于问答任务，可能会依赖于精确度（EM）和F1分数；而对于事实核查任务，则主要关注准确度。像RALLE这样的工具，专为自动评估RAG应用而设计，同样基于这些任务特定的指标进行评估。

尽管如此，专门针对RAG模型独特特性的评估研究相对较少。接下来的部分将从任务特定的评估方法和指标转向，提供基于现有文献的综合概述，这些文献基于RAG模型的独特属性。这一探索涵盖了RAG评估的目标、评估模型的方面以及可用的基准和工具。目的是提供一个全面的RAG模型评估概述，概述专门针对这些先进生成系统的独特方面的评估方法。

7.1 评估目标
RAG模型的评估主要围绕两个关键组件：检索和生成模块。这种划分确保了对提供的上下文质量和生成内容的质量进行全面评估。在检索模块中，评估的重点是检索器组件提供的上下文质量。在生成模块中，评估的重点是生成器从检索到的上下文中合成连贯和相关回答的能力。这种评估可以基于内容的目标进行分类：未标记内容和标记内容。对于未标记内容，评估包括生成回答的忠实度、相关性和无害性。对于标记内容，重点是信息的准确性。

7.2 评估方面
当代RAG模型的评估实践强调三个主要的质量分数和四个基本能力，这些共同构成了RAG模型的两个主要目标：检索和生成。质量分数包括上下文相关性、回答忠实度和回答相关性。这些质量分数从不同的角度评估RAG模型在信息检索和生成过程中的效率。上下文相关性评估检索上下文的精确度和特异性，确保相关性并最小化与无关内容相关的处理成本。回答忠实度确保生成的回答保持与检索上下文的一致性，避免矛盾。回答相关性要求生成的回答直接相关于提出的query，有效地解决核心询问。

这些评估方面的具体指标在表2中总结。需要注意的是，这些指标来源于相关工作，并不代表一个成熟或标准化的方法来量化RAG评估方面。一些评估研究还开发了定制的指标，以适应RAG模型的细微差别，但这里并未包括。

7.3 评估基准和工具
本节描述了RAG模型的评估框架，包括基准测试和自动化评估工具。这些工具提供了量化指标，不仅衡量RAG模型的性能，还增强了对模型在各种评估方面能力的理解。著名的基准测试如RGB和RECALL专注于评估RAG模型的基本能力。同时，最先进的自动化工具如RAGAS、ARES和TruLens利用LLMs来评判质量分数。

这些工具和基准测试共同构成了一个强大的框架，用于系统地评估RAG模型，如表3所总结。评估框架包括评估目标、评估方面以及定量指标。RGB和RECALL基准测试专注于生成质量，而RAGAS和ARES工具则同时关注检索质量和生成质量。TruLens工具则专注于检索质量。这些工具中的一些使用定制的定量指标，这些指标与传统指标有所不同。

评估框架：

RGB：评估检索质量，使用准确性（EM）、召回率（Recall）、精确度（Precision）、R-Rate（重新出现率）、余弦相似度（Cosine Similarity）、Hit Rate和MRR（Mean Reciprocal Rank）等指标。
RECALL：评估生成质量，使用准确性（Accuracy）、R-Rate、Hit Rate和MRR等指标。
RAGAS：评估检索质量，使用上下文相关性、忠实度和回答相关性等指标。
ARES：评估检索质量，使用上下文相关性、忠实度和回答相关性等指标。
TruLens：评估检索质量，使用上下文相关性、忠实度和回答相关性等指标。
请注意，表中的“†”代表基准测试，“‡”代表工具。带有“*”的指标表示定制的定量指标，这些指标与传统指标有所不同。请查阅相关文献以获取这些指标的具体量化公式。

这些评估工具和基准测试为RAG模型提供了一个全面的评估框架，有助于研究者和实践者理解模型在不同任务和应用场景下的表现。通过这些评估方法，可以更好地指导RAG模型的开发和优化，以满足实际应用的需求。

8 未来展望
本节探讨了RAG的三个未来发展方向：面临的挑战、多模态扩展以及RAG生态系统的发展。

8.1 RAG的未来挑战
尽管RAG技术取得了显著进展，但仍存在一些挑战需要深入研究：

上下文长度：RAG的有效性受到大语言模型（LLMs）上下文窗口大小的限制。在信息不足和信息稀释之间找到平衡至关重要。随着LLMs上下文窗口的扩展，适应这些变化对RAG提出了重要的研究问题。
鲁棒性：在检索过程中，噪声或矛盾信息的存在可能会对RAG的输出产生不利影响。提高RAG对这些挑战的抵抗力，特别是在处理对抗性或反事实输入时，正成为研究的重点。
混合方法（RAG+FT）：将RAG与微调（Fine-Tuning）结合正成为主要策略。确定RAG和微调的最佳集成方式（顺序、交替或端到端联合训练），以及如何利用参数化和非参数化的优势，是值得探索的领域。
扩展LLM角色：除了生成最终答案外，LLMs在RAG框架中也被用于检索和评估。识别进一步解锁RAG系统中LLMs潜力的方法是一个不断增长的研究方向。
规模定律：虽然LLMs的规模定律已经建立，但其在RAG中的适用性尚不确定。RAG模型的参数数量仍然落后于LLMs，是否存在一个逆规模定律（较小的模型性能优于较大的模型）特别引人关注，值得进一步研究。
生产就绪的RAG：RAG的实用性和与工程需求的契合促进了其采用。然而，提高检索效率、改善大型知识库中的文档召回率以及确保数据安全（例如防止LLMs无意中披露文档来源或元数据）是关键的工程挑战。
8.2 RAG生态系统
RAG已经超越了其最初的文本问答限制，拥抱了各种模态数据。这种扩展催生了创新的多模态模型，将RAG概念整合到各个领域：

图像：RA-CM3是一个开创性的多模态模型，能够检索和生成文本和图像。BLIP-2利用冻结的图像编码器和LLMs进行高效的视觉语言预训练，实现零样本图像到文本的转换。
音频和视频：GSS方法通过检索和拼接音频片段，将机器翻译数据转换为语音翻译数据。UEOP在端到端自动语音识别方面取得了重大进展，通过整合外部离线策略进行语音到文本的转换。
代码：RBPS在小规模学习任务中表现出色，通过编码和频率分析检索与开发者目标一致的代码示例。CoK方法首先从知识图中提取与输入query相关的事实，然后将这些事实作为提示集成到输入中，提高了知识图谱问答任务的性能。
RAG生态系统的发展受到其技术栈的进展的极大影响。随着ChatGPT的出现，LangChain和LLamaIndex等关键工具迅速获得了流行，提供了广泛的RAG相关API，并在LLM领域变得至关重要。新兴的技术栈，虽然功能不如LangChain和LLamaIndex丰富，但以其专业化的提供脱颖而出。例如，Flowise AI优先考虑低代码方法，使用户可以通过用户友好的拖放界面部署包括RAG在内的AI应用。其他技术如HayStack、Meltano和Cohere Coral也因其在该领域的特殊贡献而受到关注。

除了AI专注的提供商外，传统的软件和云服务提供商也在扩展其服务，包括RAG为中心的服务。Weaviate的Verba专为个人助理应用设计，而Amazon的Kendra提供了一个智能的企业搜索服务，允许用户使用内置连接器浏览各种内容库。在RAG技术景观的演变中，出现了明显的专业化趋势，例如：1）定制化，满足特定需求。2）简化，使RAG更易于使用，降低初始学习曲线。3）专业化，使RAG更有效地服务于生产环境。

RAG模型与其技术栈的相互成长显而易见；技术进步不断为现有基础设施设定新标准。反过来，技术栈的增强推动了RAG能力的进化。RAG工具包正在汇聚成一个基础技术栈，为先进的企业应用奠定基础。然而，一个完全集成的、全面的平台的概念仍在未来，有待进一步的创新和发展。


图7：RAG生态系统总结
9.结论
本篇综述论文总结了RAG（Retrieval-Augmented Generation）在增强大语言模型（LLMs）能力方面的显著进展，特别是在处理知识密集型任务方面。我们的分析描绘了RAG技术的三个发展阶段：Naive RAG、Advanced RAG和Modular RAG，每个阶段都在其前身的基础上进行了逐步增强。Advanced RAG阶段超越了Naive RAG，通过引入复杂的架构元素，如query重写、块重排和提示摘要，实现了更细致和模块化的架构，这增强了LLMs的性能和可解释性。RAG的技术整合与其他AI方法，如微调和强化学习，进一步扩展了其能力。在内容检索方面，一种混合方法正在兴起，利用结构化和非结构化数据源，提供了更丰富的检索过程。RAG框架内的尖端研究正在探索新概念，如从LLMs进行自我检索和动态信息检索时机。

尽管RAG技术取得了长足的进步，但在提高其鲁棒性和处理扩展上下文的能力方面仍有许多研究机会。RAG的应用范围也在向多模态领域扩展，适应其原则。随着RAG应用领域的发展，迫切需要完善评估方法以跟上其演变。确保性能评估保持准确和代表性对于捕捉RAG对AI研究和发展社区的全部贡献至关重要。

RAG生态系统的发展得到了RAG中心AI应用的增加和支持工具的持续开发的支持。然而，随着RAG应用领域的扩大，迫切需要完善评估方法以跟上其发展。确保性能评估保持准确和代表性对于捕捉RAG对AI研究和发展社区的全部贡献至关重要。RAG在增强LLMs能力方面的显著进展，通过将语言模型的参数化知识与外部知识库的广泛非参数化数据相结合，已经引起了学术界和工业界的广泛关注。随着RAG生态系统的不断增长，对评估方法的完善和支持工具的发展将为AI部署带来重要的实际意义。

一些名词解释：
query（文中的问题/查询/指令）--在自然语言处理和机器学习领域中，query通常指的是查询、询问或请求。在大模型中，query可以指代用户输入的问题或指令，用于触发模型进行相应的处理和响应。例如，在对话生成模型中，query可以是用户提出的问题，模型会根据query生成相应的回答。在其他应用中，query也可以指代用户输入的特定指令或查询条件，用于从大模型中检索相关信息。因此，在大模型中，query的具体含义取决于上下文和模型的应用场景。

Masked Language Modeling（MLM）--是一种预训练语言模型的方法。它通过在输入文本中随机掩盖一些单词或标记，并要求模型预测这些被掩盖的单词或标记，以此训练模型来学习上下文信息，提高预测准确性。MLM的主要目的是让模型能够理解语言的上下文关系，并生成合理的回复或文本。MLM可以用于各种自然语言处理任务，如文本分类、机器翻译、情感分析等。MLM的训练过程通常采用自监督学习的方式进行，这意味着模型通过分析已有的语料库来学习语言的规律和结构。
